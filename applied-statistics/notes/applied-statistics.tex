\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}			% \chapter package
\usepackage[english]{babel}
\usepackage[english]{isodate}  		% date format
\usepackage{graphicx}				% manage images
\usepackage{amsfonts}
\usepackage{booktabs}				% high quality tables
\usepackage{amsmath}				% math package
\usepackage{amssymb}				% another math package (e.g. \nexists)
\usepackage{bm}                     % bold math symbols
\usepackage{mathtools}				% emphasize equations
\usepackage{stmaryrd} 				% '\llbracket' and '\rrbracket'
\usepackage{amsthm}					% better theorems
\usepackage{enumitem}				% manage list
\usepackage{pifont}					% nice itemize
\usepackage{cancel}					% cancel math equations
\usepackage{caption}				% custom caption
\usepackage[]{mdframed}				% box text
\usepackage{multirow}				% more lines in a table
\usepackage{textcomp, gensymb}		% degree symbol
\usepackage[x11names]{xcolor}		% RGB color
\usepackage{tcolorbox}				% colorful box
\usepackage{multicol}				% more rows in a table (used for the lists)
\usepackage{url}
\usepackage{qrcode}
\usepackage{fontawesome5}
\usepackage{ragged2e}
\usepackage{cite}                   % references
\usepackage{imakeidx}               % index
\makeindex[program=makeindex, columns=2,
           title=Index, 
           intoc,
           options={-s index-style.ist}]


% draw a frame around given text
\newcommand{\framedtext}[1]{%
	\par%
	\noindent\fbox{%
		\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{#1}%
	}%
}


% table of content links
\usepackage{xcolor}
\usepackage[linkcolor=black, citecolor=blue, urlcolor=cyan]{hyperref} % hypertexnames=false
\hypersetup{
	colorlinks=true
}


\newtheorem{theorem}{\textcolor{Red3}{\underline{Theorem}}}
\renewcommand{\qedsymbol}{QED}
\newcommand{\dquotes}[1]{``#1''}
\newcommand{\longline}{\noindent\rule{\textwidth}{0.4pt}}
\newcommand{\circledtext}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt}{#1}}}}
\newcommand{\definition}[1]{\textcolor{Red3}{\textbf{#1}}\index{#1}}
\newcommand{\example}[1]{\textcolor{Green4}{\textbf{#1}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\highspace}{\vspace{1.2em}\noindent}


\begin{document}
    \newcounter{definition}[section]
    \newcounter{example}[section]
    
    \newtcolorbox[use counter = definition]{definitionbox}{%
        colback=red!5!white,
        colframe=red!75!black,
        fonttitle=\bfseries,
        title=Definition \thetcbcounter %
    }
    
    \newtcolorbox[use counter = example]{examplebox}{%
        colback=Green4!5!white,
        colframe=Green4!75!black,
        fonttitle=\bfseries,
        title=Example \thetcbcounter %
    }

    \author{260236}
	\title{Applied Statistics - Notes}
	\date{\printdayoff\today}
	\maketitle

	\newpage

    \section*{Preface}

    Every theory section in these notes has been taken from two sources:
    \begin{itemize}
        \item \href{https://www.statlearning.com/}{An Introduction to Statistical Learning}\cite{james2013introduction}
        \item Applied Multivariate Statistical Analysis (sixth edition).\cite{johnson2007applied}
    \end{itemize}
    About:
    \begin{itemize}
        \item[\faIcon{github}] \href{https://github.com/AndreVale69/HPC-E-PoliMI-university-notes}{GitHub repository}
    \end{itemize}
    
    \newpage
	
	\tableofcontents
	
	\newpage

    \section{Sample Geometry}

    \subsection{The Geometry of the Sample}

    \underline{A single} \definition{multivariate observation} is the \textbf{collection of measurements on $p$ different variables taken on the same item or trial}. If \textbf{$n$ observations} have been obtained, the entire data set can be placed in an $n \times p$ array (or matrix), also called \definition{data frame}:
    \begin{equation}\label{eq: data frame matrix}
        \underset{\left(n \times p\right)}{\mathbf{X}} = \begin{bmatrix}
            x_{11} & x_{12} & \cdots & x_{1p} \\
            x_{21} & x_{22} & \cdots & x_{2p} \\
            \vdots & \vdots & \ddots & \vdots \\
            x_{n1} & x_{n2} & \cdots & x_{np}
        \end{bmatrix}
    \end{equation}
    Each \textbf{row} of $\mathbf{X}$ represents a \textbf{multivariate observation}. Since the entire data frame is often one particular realization of what might have been observed, we say that the data frame are a \textbf{sample of size $n$ from a $p$-variate \dquotes{population}}. The sample then consists of $n$ measurements, each of which has $p$ components.

    Look at the matrix, $n$ measurements (rows), each of which has $p$ components (columns). In mathematics, each $n$ row contains $p$ columns and vice versa.
    
    \highspace
    The data frame can be plotted in two different ways:
    \begin{enumerate}
        \item $p$-dimensional scatter plot, where the rows represent $n$ points in \emph{p}-dimensional space;
        \item Geometrical representation, $p$ vectors in $n$-dimensional space.
    \end{enumerate}
    
    \longline

    \subsubsection{Scatter plot}

    For the \definition{$p$-dimensional scatter plot}, the rows of $\mathbf{X}$ represent $n$ points in $p$-dimensional space:
    \begin{equation}\label{eq: p-dimensional scatter plot}
        \underset{\left(n \times p\right)}{\mathbf{X}} = \begin{bmatrix}
            x_{11} & x_{12} & \cdots & x_{1p} \\
            x_{21} & x_{22} & \cdots & x_{2p} \\
            \vdots & \vdots & \ddots & \vdots \\
            x_{n1} & x_{n2} & \cdots & x_{np}
        \end{bmatrix} = \left[\begin{array}{@{} c @{}}
            \mathbf{x}_{1}' \\
            \mathbf{x}_{2}' \\
            \vdots \\
            \mathbf{x}_{n}'
        \end{array}\right]
        \begin{array}{l}
            \leftarrow \text{1st (multivariate) observation} \\
            \phantom{\mathbf{x}_{2}'} \\
            \phantom{\vdots} \\
            \leftarrow n\text{th (multivariate) observation}
        \end{array}
    \end{equation}
    The row vector $\mathbf{x}_{j}'$, representing the $j$th observation, contains the coordinates of a point. The \textbf{scatter plot} of $n$ points in $p$-dimensional space \textbf{provides information} on the \textbf{locations and variability of the points}.
    
    \highspace
    \underline{\textbf{Note}}: when $p$ (dimensional space) is greater than $3$, the \textbf{scatter plot} representation cannot actually be graphed. Yet the consideration of the data as $n$ points in $p$ dimensions provides \textbf{insights that are not readily available from algebraic expressions}.

    \newpage

    \subsubsection{Geometrical representation}

    The alternative \definition{geometrical representation} is constructed by considering the data as \textbf{$p$ vectors in $n$-dimensional space}. Here we take the elements of the columns of the data frame to be the coordinates of the vectors:
    \begin{equation}\label{eq: geometrical representation}
        \underset{\left(n \times p\right)}{\mathbf{X}} = \begin{bmatrix}
            x_{11} & x_{12} & \cdots & x_{1p} \\
            x_{21} & x_{22} & \cdots & x_{2p} \\
            \vdots & \vdots & \ddots & \vdots \\
            x_{n1} & x_{n2} & \cdots & x_{np}
        \end{bmatrix} = \left[\mathbf{y}_{1} \: | \: \mathbf{y}_{2} \: | \: \cdots \: | \: \mathbf{y}_{p}\right]
    \end{equation}
    Then the \textbf{coordinates} of the first point $\mathbf{y}_{1} = \left[x_{11}, x_{21}, \dots, x_{n1}\right]$ \textbf{are the $n$ measurements} on the first variable. 
    
    In general, the $i$th point $\mathbf{y}_{i} = \left[x_{11}, x_{21}, \dots, x_{n1}\right]$ is determined by the $n$-tuple of all measurements on the $i$th variable.
    
    \highspace
    \textbf{Geometrical representations} usually \textbf{facilitate understanding} and lead to further insights. The ability to \textbf{relate algebraic expressions to the geometric concepts} of length, angle and volume is therefore \textbf{very important}.
    
    \highspace
    \longline

    \subsubsection{Geometrical interpretation of the process of finding a sample mean}

    Before starting the explanation, you need to understand a few things.
    \begin{itemize}
        \item The \definition{length} of a vector $\mathbf{x}'=\left[x_{1}, x_{2}, \dots, x_{n}\right]$ with $n$ components is defined by:
        \begin{equation}\label{eq: length of a vector}
            L_{x} = \sqrt{x_{1}^{2} + x_{2}^{2} + \cdots + x_{n}^{2}}
        \end{equation}
        Multiplication of a vector $\mathbf{x}$ by a scalar $c$ changes the length:
        \begin{equation*}
            \begin{array}{rcl}
                L_{cx} &=& \sqrt{c^{2} \cdot x_{1}^{2} + c^{2} \cdot x_{2}^{2} + \cdots + c^{2} \cdot x_{n}^{2}} \\ [.3em]
                %
                &=& \left| c \right| \sqrt{x_{1}^{2} + x_{2}^{2} + \cdots + x_{n}^{2}} \\ [.3em] 
                %
                &=& \left| c \right| L_{x}
            \end{array}
        \end{equation*}
        So, for example, in $n = 2$ dimensions, the vector:
        \begin{equation*}
            \mathbf{x} = \begin{bmatrix}
                x_{1} \\ x_{2}
            \end{bmatrix}
        \end{equation*}
        The length of $\mathbf{x}$, written $L_{x}$, is defined to be:
        \begin{equation*}
            L_{x} = \sqrt{x_{1}^{2} + x_{2}^{2}}
        \end{equation*}

        \newpage

        \item Another important concept is \definition{angle}. Consider two vectors in a plane and the angle $\theta$ between them:
        \begin{figure}[!htp]
            \centering
            \includegraphics[width=.7\textwidth]{img/basics-vector-algebra-1.pdf}
            \caption{The angle $\theta$ between $\mathbf{x}' = \left[x_{1}, x_{2}\right]$ and $\mathbf{y}' = \left[y_{1}, y_{2}\right]$.}
        \end{figure}
        The value $\theta$ can be represented as the difference between the angles $\theta_{1}$ and $\theta_{2}$ formed by the two vectors and the first coordinate axis. Since, by definition:
        \begin{gather*}
            \begin{array}{rcl}
                \cos\left(\theta_{1}\right) = \dfrac{x_{1}}{L_{x}} & \cos\left(\theta_{2}\right) = \dfrac{y_{1}}{L_{y}} \\ [1em]
                %
                \sin\left(\theta_{1}\right) = \dfrac{x_{2}}{L_{x}} & \sin\left(\theta_{2}\right) = \dfrac{y_{2}}{L_{y}}
            \end{array} \\
            \cos\left(\theta\right) = \cos\left(\theta_{2} - \theta_{1}\right) = \cos\left(\theta_{2}\right) \cos\left(\theta_{1}\right) + \sin\left(\theta_{2}\right) \sin\left(\theta_{1}\right)
        \end{gather*}
        The angle $\theta$ between the two vectors $\mathbf{x}' = \left[x_{1}, x_{2}\right]$ and $\mathbf{y}' = \left[y_{1}, y_{2}\right]$ is specified by:
        \begin{equation}\label{eq: angle}
            \cos\left(\theta\right) = \cos\left(\theta_{2} - \theta_{1}\right) = 
            \left(\dfrac{y_{1}}{L_{y}}\right)\left(\dfrac{x_{1}}{L_{x}}\right) + \left(\dfrac{y_{2}}{L_{y}}\right)\left(\dfrac{x^{2}}{L_{x}}\right) =
            \dfrac{x_{1}y_{1} + x_{2}y_{2}}{L_{x}L_{y}}
        \end{equation}

        \item With the angle equation~\ref{eq: angle}, it's convenient to introduce the \definition{inner product} of two vectors:
        \begin{equation*}
            \mathbf{x}\mathbf{y}' = x_{1}y_{1} + x_{2}y_{2}
        \end{equation*}
        So let us rewrite:
        \begin{itemize}
            \item The \textbf{length} equation~\ref{eq: length of a vector}:
            \begin{equation}\label{eq: length rewritten with inner product}
                \mathbf{x}'\mathbf{x} = x_{1} x_{1} + x_{1} x_{1} = x_{1}^{2} + x_{2}^{2} 
                \longrightarrow
                L_{x} = \sqrt{x_{1}^{2} + x_{2}^{2}}
                \Longrightarrow
                L_{x} = \sqrt{\mathbf{x}' \mathbf{x}}
            \end{equation}

            \item The \textbf{angle} equation~\ref{eq: angle}:
            \begin{equation*}
                \cos\left(\theta\right) = \dfrac{x_{1}y_{1} + x_{2}y_{2}}{L_{x}L_{y}} 
                \Longrightarrow
                \cos\left(\theta\right) = \dfrac{\mathbf{x}'\mathbf{y}}{L_{x}L_{y}}
            \end{equation*}
            And using the rewritten length equation:
            \begin{equation*}
                \cos\left(\theta\right) = \dfrac{\mathbf{x}'\mathbf{y}}{L_{x}L_{y}} \Longrightarrow
                \cos\left(\theta\right) = \dfrac{\mathbf{x}'\mathbf{y}}{\sqrt{\mathbf{x}' \mathbf{x}} \cdot \sqrt{\mathbf{y}' \mathbf{y}}}
            \end{equation*}
        \end{itemize}
        
        \item The \definition{projection} (or shadow) of a vector $\mathbf{x}$ on a vector $\mathbf{y}$ is:
        \begin{equation}\label{eq: projection}
            \dfrac{\left(\mathbf{x}'\mathbf{y}\right)}{\mathbf{y}'\mathbf{y}}\mathbf{y} = \dfrac{\left(\mathbf{x}'\mathbf{y}\right)}{L_{y}}\dfrac{1}{L_{y}}\mathbf{y}
        \end{equation}
        Where the vector $\dfrac{1}{L_{y}}\mathbf{y}$ has unit length. The \textbf{length of the projection} is:
        \begin{equation}\label{eq: length of the projection}
            \dfrac{\left| \mathbf{x}'\mathbf{y} \right|}{L_{y}} = L_{x} \left| \dfrac{\mathbf{x}'\mathbf{y}}{L_{x}L_{y}} \right| = L_{x} \left| \cos\left(\theta\right) \right|
        \end{equation}
        Where $\theta$ is the angle between $\mathbf{x}$ and $\mathbf{y}$:
        \begin{figure}[!htp]
            \centering
            \includegraphics[width=.7\textwidth]{img/basics-vector-algebra-2.pdf}
            \caption{The projection of $\mathbf{x}$ on $\mathbf{y}$.}
        \end{figure}
    \end{itemize}
    Start by defining the $n \times 1$ vector $\mathbf{1}_{n}' = \left[1, 1, \dots, 1\right]$. The vector $\mathbf{1}$ forms equal angles with each of the $n$ coordinates axes, so the vector $\left(\dfrac{1}{\sqrt{n}}\right)\mathbf{1}$ has unit length in the equal-angle direction. Consider the vector $\mathbf{y}_{i}' = \left[x_{1i}, x_{2i}, \dots, x_{ni}\right]$. The projection of $\mathbf{y}_{i}$ on the unit vector $\left(\dfrac{1}{\sqrt{n}}\right)\mathbf{1}$ is:
    \begin{equation}\label{eq: sample mean - geometrical representation}
        \mathbf{y}_{i}'\left(\dfrac{1}{\sqrt{n}}\mathbf{1}\right)\dfrac{1}{\sqrt{n}}\mathbf{1} = 
        \dfrac{x_{1i} + x_{2i} + \cdots + x_{ni}}{n}\mathbf{1} = \overline{x}_{i}\mathbf{1}
    \end{equation}
    Although it may seem like a complex equation at first glance, it is nothing more than the mean! In fact, the \textbf{sample mean} $\overline{\mathbf{x}}_{i} = \dfrac{\left(x_{1i} + x_{2i} + \cdots + x_{ni}\right)}{n} = \dfrac{\mathbf{y}_{i}' \mathbf{1}}{n}$ corresponds to the multiple of $\mathbf{1}$ required to give the projection of $\mathbf{y}_{i}$ onto the line determined by $\mathbf{1}$.\newpage

    \noindent
    Furthermore, using the projection, you can obtain the \textbf{deviation} (\textbf{mean corrected}). For each $\mathbf{y}_{i}$ we have the decomposition:
    \begin{figure}[!htp]
        \centering
        \includegraphics[width=.6\textwidth]{img/basics-vector-algebra-3.pdf}
    \end{figure}

    \noindent
    Where $\overline{x}_{i} \mathbf{1}$ is perpendicular to $y_{i}-\overline{x}_{i}\mathbf{1}$. The \definition{deviation}, or \definition{mean corrected}, vector is:
    \begin{equation}\label{eq: deviation - mean corrected}
        \mathbf{d}_{i} = \mathbf{y}_{i} - \overline{x}_{i}\mathbf{1} = \begin{bmatrix}
            x_{1i} - \overline{x}_{i} \\
            x_{2i} - \overline{x}_{i} \\
            \vdots \\
            x_{ni} - \overline{x}_{i}
        \end{bmatrix}
    \end{equation}
    The \textbf{elements} of $\mathbf{d}_{i}$ are the \textbf{deviations of the measurements on the} $\bm{i}$\textbf{th variable from their sample mean}.\newline

    \noindent
    Using the length rewritten with inner product (equation~\ref{eq: length rewritten with inner product}) and the deviation (equation~\ref{eq: deviation - mean corrected}), we obtain:
    \begin{equation}
        L_{\mathbf{d}_{i}}^{2} = \mathbf{d}_{i}'\mathbf{d}_{i} = \displaystyle\sum_{j=1}^{n}\left(x_{ji}-\overline{x}_{i}\right)^{2}
    \end{equation}
    \begin{equation*}
        \left(\text{Length of deviation vector}\right)^{2} = \text{sum of squared deviations}
    \end{equation*}
    From the sample standard deviation, we see that the \textbf{squared length is proportional to the variance} of the measurements on the $i$th variable. Equivalently, the \textbf{length is proportional to the standard deviation}. So longer vectors represent more variability than shorter vectors.

    Furthermore, for any two deviation vectors $\mathbf{d}_{i}$ and $\mathbf{d}_{k}$:
    \begin{equation}
        \mathbf{d}_{i}'\mathbf{d}_{k} = \displaystyle\sum_{j=1}^{n}\left(x_{ji} - \overline{x}_{i}\right)\left(x_{jk} - \overline{x}_{k}\right)
    \end{equation}
    And with a few mathematical operations, we can get it:
    \begin{equation}
        r_{ik} = \dfrac{s_{ik}}{\sqrt{s_{ii}}\sqrt{s_{kk}}} = \cos\left(\theta_{ik}\right)
    \end{equation}
    Where the \textbf{cosine} of the angle is the \definition{sample correlation coefficient}. Note: $s_{ik}$ is the \definition{sample covariance}:
    \begin{equation}\label{eq: sample covariance}
        s_{ik} = \dfrac{1}{n} \displaystyle\sum_{j=1}^{n} \left(x_{ji} - \overline{x}_{i}\right)\left(x_{jk} - \overline{x}_{k}\right) \hspace{2em} i = 1,2,\dots,p, \hspace{1em} k = 1,2,\dots,p
    \end{equation}
    Thus:
    \begin{itemize}
        \item If the two deviation vectors have \textbf{nearly the same orientation}, the sample correlation will be close to $1$;
        \item If the two vectors are \textbf{nearly perpendicular}, the sample correlation will be approximately zero;
        \item If the two vectors are oriented in \textbf{nearly opposite directions}, the sample correlation will be close to $-1$.
    \end{itemize}

    \longline

    \subsection{Generalized Variance}

    Before starting the explanation, you need to understand what is a sample variance.

    A \definition{sample variance} is defined as:
    \begin{equation}\label{eq: sample variance}
        s_{k}^{2} = s_{kk} = \dfrac{1}{n-1} \displaystyle\sum_{j=1}^{n} \left(x_{jk} - \overline{x}_{k}\right)^{2} \hspace{2em} k = 1, 2, \dots, p
    \end{equation}

    With a single variable, the \textbf{sample variance is often used to describe the amount of variation in the measurements on that variable}. When $p$ variables are observed on each unit, the variation is described by the \definition{sample variance-covariance matrix}:
    \begin{equation}
        \mathbf{S} = \begin{bmatrix}
            s_{11} & s_{12} & \cdots & s_{1p} \\
            s_{21} & s_{22} & \cdots & s_{2p} \\
            \vdots & \vdots & \ddots & \vdots \\
            s_{p1} & s_{p2} & \cdots & s_{pp}
        \end{bmatrix} =
        \left\{s_{ik} = \dfrac{1}{n-1}\displaystyle\sum_{j=1}^{n}\left(x_{ji}-\overline{x}_{i}\right)\left(x_{jk}-\overline{x}_{k}\right)\right\}
    \end{equation}
    The sample covariance matrix contains $p$ variances and $\dfrac{1}{2}p\left(p-1\right)$ potentially different covariances. Sometimes it's desirable to \textbf{assign a single numerical value for the variation expressed by $\mathbf{S}$}. One choice for a value is the \href{https://en.wikipedia.org/wiki/Determinant}{determinant} of $\mathbf{S}$, which reduces to the usual sample variance of a single characteristic when $p=1$. This determinant is called the \definition{generalized sample variance}:
    \begin{equation}
        \text{Generalized sample variance} = \det\left(\mathbf{S}\right) = \left| \mathbf{S} \right|
    \end{equation}
    
    \newpage

    \section{Statistical Learning}

    \subsection{Introduction}

    Suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_{1}, X_{2}, \dots, X_{p}$. We assume that there is some relationship between $Y$ and $X = \left(X_{1}, X_{2}, \dots, X_{p}\right)$, which can be written in the general form:
    \begin{equation}\label{eq: error term systematic}
        Y = f\left(X\right) + \varepsilon
    \end{equation}
    Where $\varepsilon$ is an \definition{error term}, which is \textbf{independent} of $X$ and has \textbf{mean zero}. The function $f$ represents the \definition{systematic information} that $X$ provides about $Y$. The \textbf{function} $f$ that connects the input variables to the output variable \textbf{is in general unknown}.
    
    \begin{figure}[!htp]
        \begin{examplebox}
            For \example{example}, on the left-hand panel of figure~\ref{fig: error term systematic}, a plot \texttt{income} versus \texttt{years of education} for 30 individuals in the Income data set.
            
            \begin{center}
                \includegraphics[width=\textwidth]{img/error-term-systematic-1.pdf}
                \captionof{figure}{The \texttt{Income} data set.\cite{james2013introduction}}
                \label{fig: error term systematic}
            \end{center}


            \noindent
            As you can see, the plot suggests that one might be able to predict \texttt{income} using \texttt{years of education}. Since \texttt{Income} is a simulated data set, the function $f$ is known and is shown by the blue curve in the right-hand panel. The \textbf{vertical lines} represent the \textbf{error terms} $\varepsilon$. We note that some of the 30 observations lie above the blue curve and some lie below it; overall, the \textbf{errors have approximately mean zero}.
        \end{examplebox}
    \end{figure}

    \noindent
    In essence, \textbf{statistical learning refers to a set of approaches for estimating $f$}. In this chapter we outline some of the key theoretical concepts that arise in estimating $f$.

    \newpage

    \subsection{Why Estimate \emph{f} (systematic information provided by a predictor about a quantitative response)?}

    There are two main reasons that we may wish to estimate $f$\index{systematic information}: \definition{prediction} and \definition{inference}.

    \longline

    \subsubsection{Prediction}\label{subsubsection: prediction}

    In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, since the error term $\varepsilon$ averages to zero, we can predict $Y$ using:
    \begin{equation}
        \hat{Y} = \hat{f}\left(X\right)
    \end{equation}
    \begin{itemize}
        \item $\hat{f}$ represents our \textbf{estimate for} $\bm{f}$
        \item $\hat{Y}$ represents \definition{prediction} for $Y$
    \end{itemize}
    The function $\hat{f}$ is often treated as a \textbf{black box}, in the sense that one is not typically concerned with the exact form of $\hat{f}$, provided that \textbf{it yields accurate predictions for} $Y$.

    \begin{examplebox}
        As an \example{example}, suppose that:
        \begin{itemize}
            \item $X_{1}, \dots, X_{p}$ are \textbf{characteristics of a patient's blood sample} that can be easily measured in a lab.
            \item $Y$ is a variable encoding the \textbf{patient's risk for a severe adverse reaction to a particular drug}.
        \end{itemize}
        It is natural to seek to predict $Y$ using $X$, since we can then avoid giving the drug in question to patients who are at high risk of an adverse reaction. That is, patients for whom the estimate of $Y$ is high.
    \end{examplebox}

    \noindent
    The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantities: \definition{reducible error} and \definition{irreducible error}.
    \begin{itemize}
        \item In general, $\hat{f}$ will not be a perfect estimate for $f$, and this \textbf{inaccuracy} will introduce some error. This is a \definition{reducible error} because we can potentially \textbf{improve the accuracy of $\bm{\hat{f}}$ by using the most appropriate statistical learning technique to estimate $\bm{f}$}.
        
        \item Even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\hat{Y} = f\left(X\right)$, our prediction would still have some error in it! This is because $Y$ is also a function of $\varepsilon$ (error term), which, by definition, cannot be predicted using $X$. Therefore, variability associated with $\varepsilon$ also affects the accuracy of our predictions. This is the \definition{irreducible error}, because \textbf{no matter how well we estimate $\bm{f}$, we cannot reduce the error introduced by $\bm{\varepsilon}$}.
    \end{itemize}
    The real question is: \emph{why is the irreducible error larger than zero?} Well, the quantity $\varepsilon$ may contain unmeasured variables that are useful in predicting $Y$: since we don't measure them, $f$ cannot use them for its prediction. The quantity $\varepsilon$ may also contain unmeasurable variation.

    \begin{examplebox}
        For \example{example}, the risk of an adverse reaction might vary for a given patient on a given day, depending on manufacturing variation in the drug itself or the patient's general feeling of well-being on that day.
    \end{examplebox}

    \noindent
    Consider a given estimate $\hat{f}$ and a set of predictors $X$, which yields the prediction $\hat{Y} = \hat{f}\left(X\right)$. Assume for a moment that both $\hat{f}$ and $X$ are fixed, so that the only variability comes from $\varepsilon$ (error term). Then, it's easy to show that:
    \begin{equation}\label{eq: reducible and irreducible error}
        \begin{array}{rcl}
            E\left(Y - \hat{Y}\right)^{2} &=& E\left[f\left(X\right) + \varepsilon - \hat{f}\left(X\right)\right]^{2} \\ [1em]
                                          &=& \underbrace{\left[f\left(X\right) - \hat{f}\left(X\right)\right]^{2}}_{\text{Reducible}} + \underbrace{\Var\left(\varepsilon\right)}_{\text{Irreducible}}
        \end{array}
    \end{equation}
    \begin{itemize}
        \item $\left[f\left(X\right) - \hat{f}\left(X\right)\right]^{2}$ represents the \textbf{squared difference between the predicted and actual value of} $\bm{Y}$
        
        \item $E\left(Y-\hat{Y}\right)^{2}$ represents the \textbf{average}, or \definition{exprected value}
        
        \item $\Var\left(\varepsilon\right)$ represents the \definition{variance} \textbf{associated with the error term} $\bm{\varepsilon}$
    \end{itemize}
    The focus of this course is on \emph{techniques} for estimating $f$ with the aim of \textbf{minimizing the reducible error}. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$. Unfortunately, this bound is almost always unknown in practice.

    \begin{examplebox}
        Consider a company that is interested in conducting a direct-marketing campaign.

        The \emph{goal} is to identify individuals who are likely to respond positively to a mailing, based on observations of demographic variables measured on each individual.

        In this case:
        \begin{itemize}
            \item The demographic variables serve as \emph{predictors};
            \item Response to the marketing campaign (either positive or negative) serves as the \emph{outcome}.
        \end{itemize}
        The company is \underline{not} interested in obtaining a deep understanding of the relationships between each individual predictor and the response; instead, the company simply \textbf{wants to accurately predict the response using the predictors}.

        This is an example of \textbf{modeling for prediction}.
    \end{examplebox}

    \subsubsection{Inference}\index{inference}\label{subsubsection: inference}

    We are often interested in understanding the association between $Y$ (quantitative response) and $X_{1}, \dots, X_{p}$ ($p$-predictors). In this situation we wish to estimate $f$ (systematic information), but our goal is not necessarily to make predictions for $Y$. Now it's obviously that $\hat{f}$ cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in \textbf{answering the following questions}:
    \begin{itemize}
        \item \example{\emph{Which predictors are associated with the response?}} It is often the case that only a small fraction of the available predictors are substantially associated with $Y$. So, \textbf{identifying} the few \textbf{important predictors among a large set of possible variables can be extremely useful}.

        \item \example{\emph{What is the relationship between the response and each predictor?}} Larger values of the predictor are associated with larger values of $Y$. Other predictors may have the opposite relationship. The relationship between the response and the given predictor may \textbf{depend} on:
        \begin{itemize}
            \item The \textbf{complexity} of $f$;
            \item The \textbf{values of the other predictors}.
        \end{itemize}

        \item \example{\emph{Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?}} Historically, \textbf{most methods} for estimating $f$ \textbf{have} taken \textbf{linear form}. But often the true relationship is more complicated, in which case a \textbf{linear model may not provide an accurate representation} of the relationship between the input and the output variables.
    \end{itemize}

    \begin{examplebox}
        Modeling the brand of a product that a customer might purchased based on variables such as:
            \begin{itemize}
                \item Price
                \item Store
                \item Location
                \item Discount levels
                \item Competition price
            \end{itemize}
        And so forth. In this situation one might really be most interested in the \textbf{association between each variable and the probability of purchase}. For instance, \emph{to what extent is the product's price associated with sales?}

        This is an example of \textbf{modeling for inference}.
    \end{examplebox}

    \begin{figure}[!htp]
        \begin{examplebox}
            Consider the following figure:
            \begin{center}
                \includegraphics[width=\textwidth]{img/statistical-learning-1.pdf}
                \captionof{figure}{The \texttt{Advertising} data set. The plot displays \texttt{sales}, in thousands of units, as a function of \texttt{TV}, \texttt{radio}, and \texttt{newspaper} budgets, in thousands of dollars, for 200 different markets. In each plot we show the simple least squares fit of \texttt{sales} to that variable. In other words, each blue line represents a simple model that can be used to predict \texttt{sales} using \texttt{TV}, \texttt{radio}, and \texttt{newspaper}, respectively.}
            \end{center}
            One may be interested in answering questions such as:
            \begin{itemize}
                \item \emph{Which media are associated with sales?}
                \item \emph{Which media generate the biggest boost in sales?}
                \item \emph{How large of an increase in sales is associated with a given increase in TV advertising?}
            \end{itemize}
            This situation falls into the \textbf{inference model}.
        \end{examplebox}
    \end{figure}

    \subsubsection{Difference between prediction and inference}

    \begin{examplebox}
        In a real estate setting, one may seek to relate values of homes to inputs such as:
        \begin{itemize}
            \item Crime rate
            \item Zoning
            \item Distance from a river
            \item Air quality
            \item Schools
            \item Income level of community
            \item Size of houses
        \end{itemize}
        And so forth. In this case one might be interested in the association between each individual input variable and housing price. For instance, \emph{how much extra will a house be worth if it has a view of the river?} This is an \textbf{inference problem}.

        \vspace{.5em}
        But \underline{attention}! Alternatively, one may simply be interested in predicting the value of a home given its characteristics: \emph{is this house under or over valued?} And this is a \textbf{prediction problem}.
    \end{examplebox}

    So, as you can see from the example, the difference between a prediction problem and an inference problem is so small. A problem can change its nature because the ultimate goal is also changing.

    \newpage

    \subsection{How do we estimate \emph{f}?}

    We will always assume that we have observed a set of $n$ different data points. For example, in figure~\ref{fig: error term systematic} at page~\pageref{fig: error term systematic} we observed $n=30$ data points. These observations are called \definition{training data} because we will \textbf{use these observations to train, or teach, our method how to estimate} $\bm{f}$.
    
    \highspace
    Let:
    \begin{itemize}
        \item $x_{ij}$ represent the value of the $j$th predictor, or input, for observation $i$, where $i=1,2,\dots,n$ and $j=1,2,\dots,p$
        
        \item $y_{i}$ represent the response variable for the $i$th observation.
    \end{itemize}
    Then, our training data consist of:
    \begin{equation*}
        \left\{\left(x_{1}, y_{1}\right), \left(x_{2}, y_{2}\right), \dots, \left(x_{n}, y_{n}\right)\right\}
    \end{equation*}
    Where $x_{i} = \left(x_{i1}, x_{i2}, \dots, x_{ip}\right)^{T}$.
    
    \highspace
    Our \underline{goal} is to \textbf{apply a statistical learning method to the training data in order to estimate the unknown function} $\bm{f}$. In other words, we want to find a function $\hat{f}$ such that $Y \approx \hat{f}\left(X\right)$ for any observations $\left(X, Y\right)$. Most statistical learning methods for this task can be characterized as either \definition{parametric} or \definition{non-parametric}.
    
    \newpage

    \subsubsection{Parametric Methods}

    The \definition{parametric methods} involve a two-step model-based approach:
    \begin{enumerate}
        \item Select a model.
        \begin{enumerate}
            \item \textbf{Make an assumption about the functional form}, or shape, of $f$. For \example{example}, one very simple assumption is that $f$ is linear in $X$:
            \begin{equation}
                f\left(X\right) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \cdots + \beta_{p}X_{p}
            \end{equation}
            This is a \definition{linear model} (that will be discussed in the future). Once we have assumed that $f$ is linear, \textbf{the problem of estimating $\bm{f}$ is greatly simplified}. Instead of having to estimate an entirely arbitrary $p$-dimensional function $f\left(X\right)$, one only needs to \textbf{estimate the $\bm{p+1}$ coefficients} $\beta_{0}, \beta_{1}, \dots, \beta_{p}$.
        \end{enumerate}

        \item Use training data to fit/train the model.
        \begin{enumerate}
            \item[(b)] After a model has been selected, we need a \textbf{procedure that uses the training data to} \definition{fit the model} or \definition{train the model}. In the case of the linear method, we need to estimate the parameters $\beta_{0}, \beta_{1}, \dots, \beta_{p}$. So, we want to find values of these parameters such that:
            \begin{equation*}
                Y \approx \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \cdots + \beta_{p}X_{p}
            \end{equation*}
            The most \textbf{common approach to fitting} the (linear) model is referred to as (\textbf{ordinary}) \definition{least squares} (that will be discussed in the future). However, the least squares is one of many possible ways to fit the linear model.
        \end{enumerate}
    \end{enumerate}
    The parametric model-based reduces the problem of estimating $f$ down to one of \textbf{estimating a set of parameters}. In fact, assuming a parametric form for $f$ simplifies the problem of estimating $f$ because it is generally much easier to estimate a set of parameters in the linear model, than it is to fit an entirely arbitrary function $f$.

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Potential disadvantage}}
    \end{flushleft}
    The \textbf{model} we choose will \textbf{usually not match the true unknown form of} $\bm{f}$. If the chosen model is \textbf{too far} from the true $f$, then our \textbf{estimate will be poor}.

    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Possible (partial) solution}}
    \end{flushleft}
    We can try to address this problem by \textbf{choosing} \definition{flexible models} that can \textbf{fit many different possible functional forms for} $\bm{f}$. But fitting a more flexible model \textbf{requires estimating a greater number of parameters}. 
    
    These more complex models (\textbf{flexible models}) can lead to a phenomenon known as \definition{overfitting} the data, which essentially means \textbf{they follow the errors}, or \definition{noise}, \textbf{too closely} (these issues are discussed throughout this course).

    \newpage

    \subsubsection{Non-Parametric Methods}

    The \definition{non-parametric} methods do not make explicit assumptions about the functional form of $f$. Instead they seek an \textbf{estimate of} $\bm{f}$ \textbf{that gets as close to the data points as possible without being too rough or wiggly}.

    \begin{flushleft}
        \textcolor{Green3}{\faIcon{check} \textbf{Major advantage over parametric approaches}}
    \end{flushleft}
    By avoiding the assumption of a particular functional form for $f$, non-parametric approaches have the \textbf{potential to accurately fit a wider range of possible shapes} for $f$. Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$, in which case the resulting model will not fit the data well.

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Disadvantage}}
    \end{flushleft}
    Since non-parametric approaches do not reduce the problem of estimating $f$ to a small number of parameters, \textbf{a very large number of observations} (far more than is typically needed for a parametric approach) \textbf{is required in order to obtain an accurate estimate} for $f$.

    \newpage

    \subsection{Supervised and Unsupervised Learning}

    Most statistical learning problems fall into one of two categories: \definition{supervised learning} or \definition{unsupervised learning}.

    \longline

    \begin{flushleft}
        \large
        \textbf{Supervised learning}
    \end{flushleft}
    The examples that we have discussed in this chapter all fall into the \definition{supervised learning} domain. For each observation of the predictor measurement(s) $x_{i}, i=1,\dots,n$ there is an associated response measurement $y_{i}$.

    \highspace
    We wish to \textbf{fit a model that relates the response to the predictors}, with the \underline{aim} of:
    \begin{itemize}
        \item \textbf{Accurately predicting the response for future observations} (prediction, section~\ref{subsubsection: prediction})
        \item \textbf{Better understanding the relationship between the response and the predictors} (inference, section~\ref{subsubsection: inference})
    \end{itemize}

    \longline
    
    \begin{flushleft}
        \large
        \textbf{Unsupervised learning}
    \end{flushleft}
    The \definition{unsupervised learning} describes the somewhat more challenging situation in which \textbf{for every observation} $i=1,\dots,n$, \textbf{we observe a vector of measurements} $x_{i}$ \textbf{but no associated response} $y_{i}$. 
    
    \highspace
    In this setting, we are in some sense \emph{working blind}; the situation is referred to as \textbf{unsupervised} because \textbf{we lack a response variable that can supervise our analysis}. We can \textbf{seek to understand the relationships between the variables or between the observations}.

    \newpage

    \subsection{Assessing Model Accuracy}

    The aim of this section is to decide which method will give the best results for a given set of data.

    \subsubsection{Measuring the Quality of Fit (MSE)}

    In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. We need to \textbf{quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation}. The most commonly-used measure is the \definition{mean squared error (MSE)}:
    \begin{equation}\label{eq: mean squared error}
        \mathrm{MSE} = \dfrac{1}{n} \displaystyle\sum_{i=1}^{n} \left(y_{i} - \hat{f}\left(x_{i}\right)\right)^{2}
    \end{equation}
    \begin{itemize}
        \item $\hat{f}\left(x_{i}\right)$ is the prediction that $\hat{f}$ gives for the $i$th observation
        \item $y_{i}$ the $i$th true response
    \end{itemize}
    Obviously,the MSE will be:
    \begin{itemize}
        \item \textbf{Small} if the predicted responses are very close to the true responses;
        \item \textbf{Large} if for some of the observations, the predicted and true responses differ substantially.
    \end{itemize}
    In general, we do not really care how well the method works on the training data. Rather, \textbf{we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data}.

    \begin{examplebox}
        Suppose that we are interested in developing an algorithm to predict a stock's price based on previous stock returns.

        \highspace
        We can train the method using stock returns from the past 6 months. But we \underline{don't} really care how well our method predicts last week's stock price.

        We instead \textbf{care about how well it predict tomorrow's price or next month's price}.
    \end{examplebox}

    \begin{examplebox}
        Suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes.

        \highspace
        We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements.

        \highspace
        In practice, \textbf{we want this method to accurately predict diabetes risk for \emph{future patients} based on their clinical measurements}. Again, we are \underline{not} very interested in whether or not the method accurately predicts diabetes risk for patients used to train the mode, since \underline{we already know which of those patients have diabetes}!
    \end{examplebox}

    \noindent
    In mathematical terms, suppose that we fit our statistical learning method on our training observations:
    \begin{equation*}
        \left\{\left(x_{1}, y_{1}\right), \left(x_{2}, y_{2}\right), \dots, \left(x_{n}, y_{n}\right)\right\}
    \end{equation*}
    And we obtain the estimate $\hat{f}$. We can then compute:
    \begin{equation*}
        \hat{f}\left(x_{1}\right), \hat{f}\left(x_{2}\right), \dots, \hat{f}\left(x_{n}\right)
    \end{equation*}
    If these are approximately equal to:
    \begin{equation*}
        y_{1}, y_{2}, \dots, y_{n}
    \end{equation*}
    Then \textbf{the training MSE is small}.

    \highspace
    However, we are really \underline{not interested} in whether $\hat{f}\left(x_{i}\right) \approx y_{i}$; instead, we want to know whether $\hat{f}\left(x_{0}\right)$ is approximately equal to $y_{0}$, where $\left(x_{0}, y_{0}\right)$ is a \textbf{previously unseen test observation not used to train the statistical learning method}.

    We \textbf{want to choose the method that gives the lowest} \definition{test mean squared error (MSE)}, as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute:
    \begin{equation}
        \mathrm{Ave}\left(y_{0} - \hat{f}\left(x_{0}\right)\right)^{2}
    \end{equation}
    The \textbf{average squared prediction error for these test observations} $\left(x_{0}, y_{0}\right)$.

    \begin{flushleft}
        \textcolor{Red2}{\faIcon{exclamation-triangle} \textbf{Problem to find the lowest training MSE}}
    \end{flushleft}
    There is \underline{no guarantee} that the method with the lowest training MSE will also have the lowest test MSE. 
    
    The problem is that \textbf{many statistical methods specifically estimate coefficients so as to minimize the training set MSE}. For these methods, the \textbf{training set MSE can be quite small}, \textbf{but the test MSE is often much larger}.
    
    \newpage

    \begin{figure}[!htp]
        \begin{examplebox}
            \begin{center}
                \includegraphics[width=\textwidth]{img/measuring-the-quality-of-fit-1.pdf}
                \captionof{figure}{On the left: \emph{data simulated from $f$, shown in black. Three estimates of $f$ are shown: the linear regression (orange curve), and two smoothing spline fits (blue and green curves)}. Right: \emph{Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel}.\cite{james2013introduction}}
                \label{fig: measuring the quality of fit}
            \end{center}
            In the left-hand panel we have generated observations from the (error term\index{error term}) equation~\ref{eq: error term systematic} with the true $f$ given by the black curve. 
            
            The orange, blue and green curves illustrate three possible estimates for $f$ obtained using methods with increasing levels of flexibility.

            \highspace
            It is clear that as the \textbf{level of flexibility increases}, the \textbf{curves fit the observed data more closely}. 
            
            The \emph{green curve} is the most flexible and matches the data very well; however, we observe that it fits the true $f$ (shown in black) poorly because it is too wiggly. 
            
            By \textbf{adjusting the level of flexibility} of the smoothing spline fit, we can \textbf{produce many different fits to this data}.
        \end{examplebox}
    \end{figure}

    \setcounter{example}{9}
    \newpage

    \begin{examplebox}
        \begin{center}
            Referring to Figure~\ref{fig: measuring the quality of fit}
        \end{center}

        We now move on to the right-hand panel. The grey curve displays the average training MSE as a function of flexibility, or more formally the \definition{degrees of freedom}\footnote{The degrees of freedom is a \textbf{quantity that summarizes the flexibility of a curve}.}, for a number of smoothing splines.

        The orange, blue and green squares indicate the MSEs associated with the corresponding curve in the left-hand panel.

        A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve. The \emph{linear regression} is at the most restrictive end, with two degrees of freedom.

        \highspace
        The \textbf{training MSE declines monotonically as flexibility increases}. In this example, the true $f$ is non-linear, and so the orange linear fit is not flexible enough to estimate $f$ well. 
        
        The \emph{green curve} has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.

        \highspace
        The test MSE is displayed using the red curve. As with the training MSE, the test MSE initially declines as the level of flexibility increases.

        At some point, the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate $f$ the best in the left-hand panel.

        The horizontal dashed line indicates $\Var\left(\varepsilon\right)$, the \textbf{irreducible error} (eq.~\ref{eq: reducible and irreducible error}), which \textbf{corresponds to the lost achievable test MSE among all possible methods}. Hence, the smoothing spline represented by \textbf{the blue curve is close to optimal}.
    \end{examplebox}

    In the right-hand panel of figure~\ref{fig: measuring the quality of fit}, as the flexibility of the Statistical learning method increases, we observe a \textbf{monotone decrease in the training MSE and a U-shape} in the test MSE. This is a \textbf{fundamental property} of statistical learning that holds regardless of the particular data set at hand and regardless of the Statistical method being used.
    
    As model flexibility increases, the training MSE will decrease, but the test MSE may not. \textbf{When a given method yields a small training MSE but a large test MSE}, we are said to be \definition{overfitting} the data.

    \begin{flushleft}
        \textcolor{Green3}{\faIcon{question-circle} \textbf{Why does this phenomenon happen?}}
    \end{flushleft}
    This happens because our \textbf{statistical learning procedure} is working too hard to find patterns in the training data, and \textbf{may be picking up some patterns that are just caused by random chance} rather than by true properties of the unknown function $f$.

    \noindent
    So when we \emph{overfit} the training data, the \textbf{test MSE} will be \textbf{very large because the supposed patterns that the method found in the training data simply don't exist in the test data}.

    \highspace
    We almost \underline{always} expect the \textbf{training MSE to be smaller than the test MSE} because most \textbf{statistical learning methods} either directly or indirectly seek to \textbf{minimize the training MSE}. \emph{Overfitting} refers specifically to the test case in which a \textbf{less flexible model would have yielded a smaller test MSE}.

    \begin{figure}[!htp]
        \begin{examplebox}
            \begin{center}
                \includegraphics[width=\textwidth]{img/measuring-the-quality-of-fit-2.pdf}
                \captionof{figure}{Details are as in Figure~\ref{fig: measuring the quality of fit}, using a different true $f$ that is much closer to linear. In this setting, linear regression provides a very good fit to the data.\cite{james2013introduction}}
                \label{fig: measuring the quality of fit - 2}
            \end{center}

            This figure provides another \example{example} in which the true $f$ is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a \emph{U-shape} in the test MSE. 
            
            \highspace
            However, because the truth is close to linear, the \textbf{test MSE only decreases slightly before increasing again}, so that the \textbf{orange least squares fit is substantially better than the highly flexible green curve}.
        \end{examplebox}
    \end{figure}

    \begin{figure}[!htp]
        \begin{examplebox}
            \begin{center}
                \includegraphics[width=\textwidth]{img/measuring-the-quality-of-fit-3.pdf}
                \captionof{figure}{Details are as in Figure~\ref{fig: measuring the quality of fit}, using a different true $f$ that is far from linear. In this setting, linear regression provides a very poor fit to the data.\cite{james2013introduction}}
                \label{fig: measuring the quality of fit - 3}
            \end{center}

            Finally, this figure displays an \example{example} in which $f$ is highly non-linear.

            \highspace
            The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE start to increase slowly.
        \end{examplebox}
    \end{figure}












    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


    \newpage

    \bibliography{bibtex}{}
    \bibliographystyle{plain}

    \newpage

    \printindex
\end{document}